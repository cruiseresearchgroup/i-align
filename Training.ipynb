{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "import platform\n",
    "import time\n",
    "import rdflib\n",
    "import random\n",
    "from statistics import mean\n",
    "from scipy.spatial import distance\n",
    "import scipy\n",
    "import networkx\n",
    "\n",
    "print('Python version: %s' % platform.python_version())\n",
    "print('Tensorflow version: %s' % tf.__version__)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "DEVICE = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=DEVICE\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KG data\n",
    "KG1_ENTITIES = pickle.load(open('data/KG1_ENTITIES', \"rb\"))\n",
    "KG2_ENTITIES = pickle.load(open('data/KG2_ENTITIES', \"rb\"))\n",
    "KG1_ATTRIBUTES = pickle.load(open('data/KG1_ATTRIBUTES', \"rb\"))\n",
    "KG2_ATTRIBUTES = pickle.load(open('data/KG2_ATTRIBUTES', \"rb\"))\n",
    "\n",
    "# Vocab\n",
    "KG1_ent_vocab = pickle.load(open('data/KG1_ent_vocab', \"rb\"))\n",
    "KG2_ent_vocab = pickle.load(open('data/KG2_ent_vocab', \"rb\"))\n",
    "pred_vocab = pickle.load(open('data/pred_vocab', \"rb\"))\n",
    "char_vocab = pickle.load(open('data/char_vocab', \"rb\"))\n",
    "\n",
    "seed_data = pickle.load(open('data/seed_data', \"rb\"))\n",
    "test_data = pickle.load(open('data/test_data', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_EPOCHS = 400\n",
    "BUFFER_SIZE = seed_data.shape[0]\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "CHR_EMBED_DIM = 64\n",
    "HIDDEN_UNITS = 256\n",
    "NUM_LAYERS_ATT = 4\n",
    "NUM_LAYERS_REL = 4\n",
    "NUM_HEAD = 8\n",
    "\n",
    "MAX_CHARS = 10\n",
    "MARGIN_LOSS = 1\n",
    "\n",
    "VOCAB_ENT_SIZE = len(KG1_ent_vocab) + len(KG2_ent_vocab) + 1\n",
    "VOCAB_PRE_SIZE = len(pred_vocab) + 1\n",
    "VOCAB_CHR_SIZE = len(char_vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_sample(kg2_ents):\n",
    "    return np.array(list(set(random.choices(population=list(KG2_ENTITIES.keys()), k=len(kg2_ents)*2)))[:len(kg2_ents)])\n",
    "\n",
    "def get_attribute_data(entities, KG_attribute):\n",
    "    pred_data = list()\n",
    "    attr_data = list()\n",
    "    \n",
    "    for ent in entities:\n",
    "        tmp_pred=list()\n",
    "        tmp_attr=list()\n",
    "        for pred, attr in KG_attribute[ent]:\n",
    "            tmp_pred.append(pred)\n",
    "            tmp_attr.append(attr)\n",
    "        pred_data.append(tmp_pred)\n",
    "        attr_data.append(tmp_attr)\n",
    "    return np.array(pred_data), np.array(attr_data)\n",
    "\n",
    "def get_subgraph(entities, KG_entities):\n",
    "    assert len(entities) == len(set(entities))\n",
    "        \n",
    "    # create graph\n",
    "    G = networkx.Graph()\n",
    "    \n",
    "    #add the batch nodes (needs to be separated, so that we make sure that the first N (N=batch_size) are the mini-batch nodes)\n",
    "    for e in entities:\n",
    "        G.add_node(e)\n",
    "        G.add_weighted_edges_from([(e,e,pred_vocab[\"SELF-REL\"])])\n",
    "    \n",
    "    #add the neighbors\n",
    "    for e in entities:\n",
    "        neighbours = KG_entities[e]\n",
    "        for p, n in neighbours:\n",
    "            G.add_weighted_edges_from([(e,n,p)])   \n",
    "    adj_matrix = networkx.adjacency_matrix(G)\n",
    "    \n",
    "    return np.array(G.nodes), np.array(adj_matrix.todense())\n",
    "            \n",
    "\n",
    "def get_batch_data(ent_pair):\n",
    "    kg1_ents = ent_pair[:,0]\n",
    "    kg2_ents = ent_pair[:,1]\n",
    "    neg_ents = get_neg_sample(kg2_ents)\n",
    "    assert len(kg1_ents) == len(set(kg1_ents))\n",
    "    assert len(kg2_ents) == len(set(kg2_ents))\n",
    "    assert len(neg_ents) == len(set(neg_ents))\n",
    "    \n",
    "    # get attribute data\n",
    "    kg1_attr_keys, kg1_attr_vals = get_attribute_data(kg1_ents, KG1_ATTRIBUTES)\n",
    "    kg2_attr_keys, kg2_attr_vals = get_attribute_data(kg2_ents, KG2_ATTRIBUTES)\n",
    "    neg_attr_keys, neg_attr_vals = get_attribute_data(neg_ents, KG2_ATTRIBUTES)\n",
    "    \n",
    "    # get graph data\n",
    "    kg1_node_list, kg1_adj_matrix = get_subgraph(kg1_ents, KG1_ENTITIES)\n",
    "    kg2_node_list, kg2_adj_matrix = get_subgraph(kg2_ents, KG2_ENTITIES)\n",
    "    neg_node_list, neg_adj_matrix = get_subgraph(neg_ents, KG2_ENTITIES)\n",
    "    \n",
    "    # group data\n",
    "    kg1_data = (kg1_attr_keys, kg1_attr_vals, kg1_node_list, kg1_adj_matrix)\n",
    "    kg2_data = (kg2_attr_keys, kg2_attr_vals, kg2_node_list, kg2_adj_matrix)\n",
    "    neg_data = (neg_attr_keys, neg_attr_vals, neg_node_list, neg_adj_matrix)\n",
    "    \n",
    "    return kg1_data, kg2_data, neg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = tf.data.Dataset.from_tensor_slices((seed_data))\n",
    "train_batch = tf.data.Dataset.batch(train_batch, batch_size = BATCH_SIZE, drop_remainder=True)\n",
    "for (batch, ent_pair) in enumerate(train_batch):\n",
    "    kg1_data, kg2_data, neg_data = get_batch_data(ent_pair.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    return tf.keras.layers.GRU(units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        self.key_size = HIDDEN_UNITS // self.num_head\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(HIDDEN_UNITS)\n",
    "        self.wk = tf.keras.layers.Dense(HIDDEN_UNITS)\n",
    "        self.wv = tf.keras.layers.Dense(HIDDEN_UNITS)\n",
    "        self.wn = tf.keras.layers.Dense(HIDDEN_UNITS)\n",
    "        self.wp = tf.keras.layers.Dense(HIDDEN_UNITS)\n",
    "\n",
    "    def call(self, query, value, mask=None, bias=None):\n",
    "        query = self.wq(query)\n",
    "        key = self.wk(value)\n",
    "        value = self.wv(value)\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # for parallel multihead computation\n",
    "        query = tf.reshape(query, [batch_size, -1, self.num_head, self.key_size])\n",
    "        query = tf.transpose(query, [0, 2, 1, 3])\n",
    "        key = tf.reshape(key, [batch_size, -1, self.num_head, self.key_size])\n",
    "        key = tf.transpose(key, [0, 2, 1, 3])\n",
    "        value = tf.reshape(value, [batch_size, -1, self.num_head, self.key_size])\n",
    "        value = tf.transpose(value, [0, 2, 1, 3])\n",
    "        \n",
    "        #(batch, h, query_len, value_len)\n",
    "        score = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(tf.dtypes.cast(self.key_size, dtype=tf.float32))\n",
    "        if bias is not None:\n",
    "            score = score + bias\n",
    "        if mask is not None:\n",
    "            if len(mask.shape) < 4:\n",
    "                mask = tf.expand_dims(mask, axis=3)\n",
    "                mask = tf.transpose(mask, [0, 2, 1, 3])\n",
    "            score *= mask\n",
    "            score = tf.where(tf.equal(score, 0), tf.ones_like(score) * -1e9, score)\n",
    "        \n",
    "        attention = tf.nn.softmax(score, axis=-1)\n",
    "        context = tf.matmul(attention, value)\n",
    "        context = tf.transpose(context, [0, 2, 1, 3])\n",
    "        context = tf.reshape(context, [batch_size, -1, self.key_size * self.num_head])\n",
    "        \n",
    "        \n",
    "        pred_output = score\n",
    "        pred_output = tf.reduce_sum(tf.transpose(pred_output, [2,3,1,0]), axis=-1)\n",
    "        \n",
    "        pred_output = self.wp(pred_output)\n",
    "        node_output = self.wn(context)\n",
    "        \n",
    "        return node_output, pred_output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOOKER\n",
    "class HistoricalEmbeddings(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(HistoricalEmbeddings, self).__init__()\n",
    "        \n",
    "        self.batch_embed = tf.keras.layers.Embedding(VOCAB_ENT_SIZE, \n",
    "                                                   HIDDEN_UNITS, \n",
    "                                                   mask_zero=False, \n",
    "                                                   trainable=True,\n",
    "                                                   name=\"node_batch_embedding\")\n",
    "        \n",
    "        self.hist_embed = tf.keras.layers.Embedding(VOCAB_ENT_SIZE, \n",
    "                                                   HIDDEN_UNITS, \n",
    "                                                   mask_zero=False, \n",
    "                                                   trainable=True,\n",
    "                                                   name=\"node_hist_embedding\")\n",
    "        \n",
    "        self.lin_transform_batch_1 = tf.keras.layers.Dense(HIDDEN_UNITS)\n",
    "        self.lin_transform_batch_2 = tf.keras.layers.Dense(HIDDEN_UNITS)\n",
    "        self.lin_transform_batch_3 = tf.keras.layers.Dense(HIDDEN_UNITS)\n",
    "        self.lin_transform_batch_4 = tf.keras.layers.Dense(HIDDEN_UNITS)\n",
    "        \n",
    "        self.lin_transform_hist_1 = tf.keras.layers.Dense(HIDDEN_UNITS)\n",
    "        self.lin_transform_hist_2 = tf.keras.layers.Dense(HIDDEN_UNITS)\n",
    "        self.lin_transform_hist_3 = tf.keras.layers.Dense(HIDDEN_UNITS)\n",
    "        self.lin_transform_hist_4 = tf.keras.layers.Dense(HIDDEN_UNITS)\n",
    "    \n",
    "    def call(self, ent_list):\n",
    "        \n",
    "        batch_node = ent_list[:BATCH_SIZE]\n",
    "        batch_node_emb = self.batch_embed(batch_node)\n",
    "        batch_node_emb = self.lin_transform_batch_1(batch_node_emb)\n",
    "        batch_node_emb = self.lin_transform_batch_2(batch_node_emb)\n",
    "        batch_node_emb = self.lin_transform_batch_3(batch_node_emb)\n",
    "        batch_node_emb = self.lin_transform_batch_4(batch_node_emb)\n",
    "        \n",
    "        hist_node = ent_list[BATCH_SIZE:]\n",
    "        hist_node_emb = self.hist_embed(hist_node)\n",
    "        hist_node_emb = self.lin_transform_hist_1(hist_node_emb)\n",
    "        hist_node_emb = self.lin_transform_hist_2(hist_node_emb)\n",
    "        hist_node_emb = self.lin_transform_hist_3(hist_node_emb)\n",
    "        hist_node_emb = self.lin_transform_hist_4(hist_node_emb)\n",
    "        \n",
    "        \n",
    "        entities = tf.concat([batch_node_emb, hist_node_emb], axis=0) #(NUM_NODES, 128)\n",
    "        return entities\n",
    "    \n",
    "    def get_batch_embed(self, ent_list):\n",
    "        batch_node = ent_list[:BATCH_SIZE]\n",
    "        batch_node_emb = self.batch_embed(batch_node)\n",
    "        batch_node_emb = self.lin_transform_batch_1(batch_node_emb)\n",
    "        batch_node_emb = self.lin_transform_batch_2(batch_node_emb)\n",
    "        batch_node_emb = self.lin_transform_batch_3(batch_node_emb)\n",
    "        batch_node_emb = self.lin_transform_batch_4(batch_node_emb)\n",
    "        return batch_node_emb\n",
    "    \n",
    "    def get_hist_embed(self, ent_list):\n",
    "        hist_node = ent_list[:BATCH_SIZE]\n",
    "        hist_node_emb = self.hist_embed(hist_node)\n",
    "        hist_node_emb = self.lin_transform_hist_1(hist_node_emb)\n",
    "        hist_node_emb = self.lin_transform_hist_2(hist_node_emb)\n",
    "        hist_node_emb = self.lin_transform_hist_3(hist_node_emb)\n",
    "        hist_node_emb = self.lin_transform_hist_4(hist_node_emb)\n",
    "        return hist_node_emb\n",
    "    \n",
    "    \n",
    "hist_embed = HistoricalEmbeddings()\n",
    "\n",
    "for (batch, ent_pair) in enumerate(train_batch):\n",
    "    kg1_data, kg2_data, neg_data = get_batch_data(ent_pair.numpy())\n",
    "    \n",
    "    kg1_attr_keys, kg1_attr_vals, kg1_node_list, kg1_adj_matrix = kg1_data\n",
    "    kg2_attr_keys, kg2_attr_vals, kg2_node_list, kg2_adj_matrix = kg2_data\n",
    "    neg_attr_keys, neg_attr_vals, neg_node_list, neg_adj_matrix = neg_data\n",
    "    \n",
    "    kg1_ents = hist_embed(kg1_node_list)\n",
    "    kg2_ents = hist_embed(kg2_node_list)\n",
    "    neg_ents = hist_embed(neg_node_list)\n",
    "        \n",
    "    break\n",
    "\n",
    "print('Shape of kg1_ents: %s' % kg1_ents.shape)\n",
    "print('Shape of kg2_ents: %s' % kg2_ents.shape)\n",
    "print('Shape of neg_ents: %s' % neg_ents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttAggregator(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttAggregator, self).__init__()\n",
    "        \n",
    "        self.W1 = tf.keras.layers.Dense(HIDDEN_UNITS)\n",
    "        self.W2 = tf.keras.layers.Dense(HIDDEN_UNITS)\n",
    "        \n",
    "        self.self_att_obj = [MultiHeadAttention(NUM_HEAD) for _ in range(NUM_LAYERS_REL)]\n",
    "        self.self_att_obj_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(NUM_LAYERS_REL)]\n",
    "        self.self_att_obj_norm = [tf.keras.layers.LayerNormalization(epsilon=1e-6) for _ in range(NUM_LAYERS_REL)]\n",
    "        \n",
    "        self.lin_transform_1_o = [tf.keras.layers.Dense(HIDDEN_UNITS * NUM_LAYERS_ATT, activation='relu') for _ in range(NUM_LAYERS_ATT)]\n",
    "        self.lin_transform_2_o = [tf.keras.layers.Dense(HIDDEN_UNITS) for _ in range(NUM_LAYERS_ATT)]\n",
    "        self.dropout_o = [tf.keras.layers.Dropout(0.1) for _ in range(NUM_LAYERS_ATT)]\n",
    "        self.batch_norm_o = [tf.keras.layers.LayerNormalization(epsilon=1e-6) for _ in range(NUM_LAYERS_ATT)]\n",
    "        \n",
    "        self.Wc = tf.keras.layers.Dense(HIDDEN_UNITS, activation='relu')\n",
    "        self.final_transform = MultiHeadAttention(NUM_HEAD)\n",
    "        \n",
    "    def call(self, predicates, objects, mask=None):\n",
    "\n",
    "        residual = tf.nn.tanh(self.W1(objects) + self.W2(predicates))\n",
    "        for i in range(NUM_LAYERS_REL):\n",
    "            \n",
    "            self_att_output, _, _ = self.self_att_obj[i](residual, residual, mask)\n",
    "            self_att_output = self.self_att_obj_dropout[i](self_att_output)\n",
    "            self_att_output = residual + self_att_output\n",
    "            self_att_output = self.self_att_obj_norm[i](self_att_output)\n",
    "            \n",
    "            output = self.lin_transform_2_o[i](self.lin_transform_1_o[i](self_att_output))\n",
    "            output = self.dropout_o[i](output)\n",
    "            output = output + residual\n",
    "            output = self.batch_norm_o[i](output)\n",
    "            \n",
    "            residual = output\n",
    "        \n",
    "        ctx = tf.reduce_sum(output, axis=1, keepdims=True)\n",
    "        ctx = self.Wc(ctx)\n",
    "        final_output, _, attention = self.final_transform(ctx, output)\n",
    "        attention = tf.reduce_mean(attention, axis=1)\n",
    "        \n",
    "        return tf.reshape(final_output, [final_output.shape[0], final_output.shape[-1]]), tf.reshape(attention, [attention.shape[0], attention.shape[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransGNN(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TransGNN, self).__init__()\n",
    "        self.pred_lin_trans_sigmoid = [tf.keras.layers.Dense(1, activation=\"sigmoid\") for _ in range(NUM_LAYERS_REL)]\n",
    "        self.pred_lin_trans_bias = [tf.keras.layers.Dense(1) for _ in range(NUM_LAYERS_REL)]\n",
    "        \n",
    "        self.self_att_nodes = [MultiHeadAttention(NUM_HEAD) for _ in range(NUM_LAYERS_REL)]\n",
    "        self.self_att_nodes_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(NUM_LAYERS_REL)]\n",
    "        self.self_att_nodes_norm = [tf.keras.layers.LayerNormalization(epsilon=1e-6) for _ in range(NUM_LAYERS_REL)]\n",
    "        self.self_att_preds_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(NUM_LAYERS_REL)]\n",
    "        self.self_att_preds_norm = [tf.keras.layers.LayerNormalization(epsilon=1e-6) for _ in range(NUM_LAYERS_REL)]\n",
    "        \n",
    "        self.lin_transform_1_o = [tf.keras.layers.Dense(HIDDEN_UNITS * NUM_LAYERS_ATT, activation='relu') for _ in range(NUM_LAYERS_ATT)]\n",
    "        self.lin_transform_2_o = [tf.keras.layers.Dense(HIDDEN_UNITS) for _ in range(NUM_LAYERS_ATT)]\n",
    "        self.dropout_o = [tf.keras.layers.Dropout(0.1) for _ in range(NUM_LAYERS_ATT)]\n",
    "        self.batch_norm_o = [tf.keras.layers.LayerNormalization(epsilon=1e-6) for _ in range(NUM_LAYERS_ATT)]\n",
    "        \n",
    "    #node_list #(NUM_NODES) --> the first N (N=batch_size) are the mini-batch nodes, the others are the one-hop nodes\n",
    "    #pred_embed #(NUM_NODES, NUM_NODES, 128)\n",
    "    #adj_mask #(NUM_NODES, NUM_NODES, 1)\n",
    "    def call(self, node_list, pred_embed, adj_mask, mask=None):\n",
    "        \n",
    "        hist_embeddings = tf.expand_dims(hist_embed(node_list), axis=0) # (1, NUM_NODES, 128)\n",
    "        residual_node = hist_embeddings # first layer, all embeddings are come from the historical embeddings\n",
    "        residual_pred = pred_embed\n",
    "        \n",
    "        hidden_states = list()\n",
    "        \n",
    "        for i in range(NUM_LAYERS_REL):\n",
    "            \n",
    "            pred_gate = self.pred_lin_trans_sigmoid[i](residual_pred) * adj_mask\n",
    "            pred_gate = tf.expand_dims(tf.transpose(pred_gate, [2, 0, 1]), axis=0)\n",
    "            pred_bias = self.pred_lin_trans_bias[i](residual_pred) * adj_mask\n",
    "            pred_bias = tf.expand_dims(tf.transpose(pred_bias, [2, 0, 1]), axis=0)\n",
    "            \n",
    "            node_output, pred_output, attention = self.self_att_nodes[i](residual_node, residual_node, mask=pred_gate, bias=None)\n",
    "            \n",
    "            node_output = node_output\n",
    "            node_output = self.self_att_nodes_dropout[i](node_output)\n",
    "            node_output = residual_node + node_output\n",
    "            node_output = self.self_att_nodes_norm[i](node_output)\n",
    "            \n",
    "            pred_output = self.self_att_preds_dropout[i](pred_output)\n",
    "            pred_output = residual_pred + pred_output\n",
    "            pred_output = self.self_att_preds_norm[i](pred_output)\n",
    "            \n",
    "            output = self.lin_transform_2_o[i](self.lin_transform_1_o[i](node_output))\n",
    "            output = self.dropout_o[i](output)\n",
    "            output = output + residual_node\n",
    "            output = self.batch_norm_o[i](output)\n",
    "            \n",
    "            residual_node = output\n",
    "            residual_pred = pred_output\n",
    "            \n",
    "            hidden_states.append(residual_node)\n",
    "        \n",
    "        final_output = residual_node\n",
    "        attention = tf.reduce_mean(attention, axis=1)\n",
    "        return tf.reshape(final_output[:,:BATCH_SIZE,:], [BATCH_SIZE, final_output.shape[-1]]), hidden_states, tf.reshape(attention[:,:BATCH_SIZE,:], [BATCH_SIZE, attention.shape[-1]]) # return only the batch node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ENCODER\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.chr_embed = tf.keras.layers.Embedding(VOCAB_CHR_SIZE, \n",
    "                                                   CHR_EMBED_DIM, \n",
    "                                                   mask_zero=False, \n",
    "                                                   trainable=True,\n",
    "                                                   name=\"source_char_embedding\")\n",
    "        self.pre_embed = tf.keras.layers.Embedding(VOCAB_PRE_SIZE, \n",
    "                                                   HIDDEN_UNITS, \n",
    "                                                   mask_zero=False, \n",
    "                                                   trainable=True,\n",
    "                                                   name=\"source_predicate_embedding\")\n",
    "        self.gru_char = gru(HIDDEN_UNITS)\n",
    "        self.trans_att = AttAggregator()\n",
    "        self.trans_rel = TransGNN()\n",
    "    \n",
    "    def call(self, att_keys, att_values, node_list, adj_matrix):\n",
    "\n",
    "        # process attribute data\n",
    "        char_x = tf.unstack(att_values, axis=1)\n",
    "        char_x_embed = list()\n",
    "        for t in range(len(char_x)):\n",
    "            mask_char = tf.expand_dims(1 - tf.cast(tf.equal(char_x[t], 0), dtype=tf.float32), axis=2)\n",
    "            char_xt = self.chr_embed(char_x[t])\n",
    "            char_xt, __ = self.gru_char(char_xt)\n",
    "            char_xt = char_xt * mask_char\n",
    "            char_xt = tf.reduce_sum(char_xt, axis=1)\n",
    "            char_x_embed.append(char_xt)\n",
    "        \n",
    "        mask_att = tf.expand_dims(1 - tf.cast(tf.equal(att_keys, 0), dtype=tf.float32), axis=2) #(BATCH_SIZE, 20, 1)\n",
    "        att_values = tf.stack(char_x_embed, axis = 1) #(BATCH_SIZE, 20, 128)\n",
    "        att_keys = self.pre_embed(att_keys) #(BATCH_SIZE, 20, 128)\n",
    "        att_rep, att_attention = self.trans_att(att_keys, att_values, mask_att) # entity representation based on attributes #(BATCH_SIZE, 128)\n",
    "        \n",
    "        \n",
    "        # process graph data\n",
    "        pred_embed = self.pre_embed(adj_matrix) #(NUM_NODES, NUM_NODES, 128)\n",
    "        adj_mask = tf.expand_dims(1 - tf.cast(tf.equal(adj_matrix, 0), dtype=tf.float32), axis = 2) #(NUM_NODES, NUM_NODES, 1)\n",
    "        node_rep, hidden_states, node_attention = self.trans_rel(node_list, pred_embed, adj_mask) # entity representation based on structures\n",
    "        \n",
    "        return att_rep, node_rep, hidden_states, att_attention, node_attention\n",
    "    \n",
    "    \n",
    "encoder = Encoder()\n",
    "\n",
    "for (batch, ent_pair) in enumerate(train_batch):\n",
    "    kg1_data, kg2_data, neg_data = get_batch_data(ent_pair.numpy())\n",
    "    \n",
    "    kg1_attr_keys, kg1_attr_vals, kg1_node_list, kg1_adj_matrix = kg1_data\n",
    "    kg2_attr_keys, kg2_attr_vals, kg2_node_list, kg2_adj_matrix = kg2_data\n",
    "    neg_attr_keys, neg_attr_vals, neg_node_list, neg_adj_matrix = neg_data\n",
    "    \n",
    "    kg1_att_embed, kg1_node_embed, kg1_states, kg1_att_attention, kg1_node_attention = encoder(kg1_attr_keys, kg1_attr_vals, kg1_node_list, kg1_adj_matrix)\n",
    "    kg2_att_embed, kg2_node_embed, kg2_states, kg2_att_attention, kg2_node_attention = encoder(kg2_attr_keys, kg2_attr_vals, kg2_node_list, kg2_adj_matrix)\n",
    "    neg_att_embed, neg_node_embed, neg_states, neg_att_attention, neg_node_attention = encoder(neg_attr_keys, neg_attr_vals, neg_node_list, neg_adj_matrix)\n",
    "    break\n",
    "\n",
    "print('Shape of kg1_att_embed: %s' % kg1_att_embed.shape)\n",
    "print('Shape of kg2_att_embed: %s' % kg2_att_embed.shape)\n",
    "print('Shape of neg_att_embed: %s' % neg_att_embed.shape)\n",
    "\n",
    "print('Shape of kg1_node_embed: %s' % kg1_node_embed.shape)\n",
    "print('Shape of kg2_node_embed: %s' % kg2_node_embed.shape)\n",
    "print('Shape of neg_node_embed: %s' % neg_node_embed.shape)\n",
    "\n",
    "print('Shape of kg1_att_attention: %s' % kg1_att_attention.shape)\n",
    "print('Shape of kg2_att_attention: %s' % kg2_att_attention.shape)\n",
    "print('Shape of neg_att_attention: %s' % neg_att_attention.shape)\n",
    "\n",
    "print('Shape of kg1_node_attention: %s' % kg1_node_attention.shape)\n",
    "print('Shape of kg2_node_attention: %s' % kg2_node_attention.shape)\n",
    "print('Shape of neg_node_attention: %s' % neg_node_attention.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_loss(real, pred):\n",
    "    real = tf.math.l2_normalize(real,1)        \n",
    "    pred = tf.math.l2_normalize(pred,1)\n",
    "    cos_sim = tf.reduce_sum(tf.multiply(real, pred), 1, keepdims=True)\n",
    "    sim_loss = tf.reduce_sum(1-cos_sim)\n",
    "    return sim_loss\n",
    "\n",
    "def margin_based_ranking_loss(x, pos_sample, neg_sample):\n",
    "    pos = tf.reduce_sum(abs(x - pos_sample), 1, keepdims = True)\n",
    "    neg = tf.reduce_sum(abs(x - neg_sample), 1, keepdims = True)\n",
    "    loss = tf.reduce_sum(tf.maximum(pos - neg + MARGIN_LOSS, 0))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_test_embeddings():\n",
    "    ### GET TEST DATA ###\n",
    "    test_batch = tf.data.Dataset.from_tensor_slices((test_data))\n",
    "    test_batch = tf.data.Dataset.batch(test_batch, batch_size = BATCH_SIZE, drop_remainder=True)\n",
    "    ### ============= ###\n",
    "\n",
    "    ### GET ALL KG1 and KG2 EMBEDDING ###\n",
    "    KG1_embed = None\n",
    "    KG2_embed = None\n",
    "    KG1_att_atts = None\n",
    "    KG2_att_atts = None\n",
    "    KG1_node_atts = None\n",
    "    KG2_node_atts = None\n",
    "    included_kg2_ent = set()\n",
    "    for (batch, ent_pair) in enumerate(test_batch):\n",
    "        kg1_data, kg2_data, _ = get_batch_data(ent_pair.numpy())\n",
    "        \n",
    "        kg1_attr_keys, kg1_attr_vals, kg1_node_list, kg1_adj_matrix = kg1_data\n",
    "        kg2_attr_keys, kg2_attr_vals, kg2_node_list, kg2_adj_matrix = kg2_data\n",
    "        \n",
    "        kg1_att_embed, kg1_node_embed, _, kg1_att_attention, kg1_node_attention = encoder(kg1_attr_keys, kg1_attr_vals, kg1_node_list, kg1_adj_matrix)\n",
    "        kg2_att_embed, kg2_node_embed, _, kg2_att_attention, kg2_node_attention = encoder(kg2_attr_keys, kg2_attr_vals, kg2_node_list, kg2_adj_matrix)\n",
    "    \n",
    "        if batch == 0:\n",
    "            KG1_embed = tf.concat([kg1_att_embed], axis=-1)\n",
    "            KG2_embed = tf.concat([kg2_att_embed], axis=-1)\n",
    "            KG1_att_atts = tf.unstack(kg1_att_attention, axis = 0)\n",
    "            KG2_att_atts = tf.unstack(kg2_att_attention, axis = 0)\n",
    "            KG1_node_atts = tf.unstack(kg1_node_attention, axis = 0)\n",
    "            KG2_node_atts = tf.unstack(kg2_node_attention, axis = 0)\n",
    "        else:\n",
    "            KG1_embed = np.concatenate([KG1_embed, tf.concat([kg1_att_embed], axis=-1)], axis=0)\n",
    "            KG2_embed = np.concatenate([KG2_embed, tf.concat([kg2_att_embed], axis=-1)], axis=0)\n",
    "            KG1_att_atts = KG1_att_atts + tf.unstack(kg1_att_attention, axis = 0)\n",
    "            KG2_att_atts = KG2_att_atts + tf.unstack(kg2_att_attention, axis = 0)\n",
    "            KG1_node_atts = KG1_node_atts + tf.unstack(kg1_node_attention, axis = 0)\n",
    "            KG2_node_atts = KG2_node_atts + tf.unstack(kg2_node_attention, axis = 0)\n",
    "    ### ===================== ###\n",
    "\n",
    "    ### GET REMAINING KG2 EMBEDDING (which is from the seed_data) ###\n",
    "    for (batch, ent_pair) in enumerate(train_batch):\n",
    "        _, kg2_data, _ = get_batch_data(ent_pair.numpy())\n",
    "        kg2_attr_keys, kg2_attr_vals, kg2_node_list, kg2_adj_matrix = kg2_data\n",
    "        kg2_att_embed, kg2_node_embed, _, kg2_att_attention, kg2_node_attention = encoder(kg2_attr_keys, kg2_attr_vals, kg2_node_list, kg2_adj_matrix)\n",
    "        KG2_embed = np.concatenate([KG2_embed, tf.concat([kg2_att_embed], axis=-1)], axis=0)\n",
    "        KG2_att_atts = KG2_att_atts + tf.unstack(kg2_att_attention, axis = 0)\n",
    "        KG2_node_atts = KG2_node_atts + tf.unstack(kg2_node_attention, axis = 0)\n",
    "    ### =========================== ###\n",
    "    attentions = (KG1_att_atts, KG2_att_atts, KG1_node_atts, KG2_node_atts)\n",
    "    return KG1_embed, KG2_embed, attentions\n",
    "    \n",
    "\n",
    "def test():\n",
    "    print (\"compute embeddings... \")\n",
    "    KG1_embed, KG2_embed, _ = get_test_embeddings()\n",
    "    \n",
    "    print (\"compute similarity... \")\n",
    "    #sim = scipy.spatial.distance.cdist(KG1_embed, KG2_embed, metric='cosine')\n",
    "    sim = scipy.spatial.distance.cdist(KG1_embed, KG2_embed, metric='cityblock')\n",
    "    print (\"sorting results... \")\n",
    "    sim = sim.argsort()\n",
    "    \n",
    "    hits_1 = list()\n",
    "    hits_10 = list()\n",
    "\n",
    "    for (idx, similarity) in enumerate(sim):\n",
    "        top_res = similarity\n",
    "\n",
    "        if idx in top_res[:1]:\n",
    "            hits_1.append(1)\n",
    "        else:\n",
    "            hits_1.append(0)\n",
    "        if idx in top_res[:10]:\n",
    "            hits_10.append(1)\n",
    "        else:\n",
    "            hits_10.append(0)\n",
    "    return mean(hits_1), mean(hits_10)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "BETA = 0.01\n",
    "GAMMA = 1e-4\n",
    "#lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=LR,decay_steps=10000,decay_rate=0.9)\n",
    "#optimizer = tf.keras.optimizers.Adam(lr_schedule, clipnorm=1.)\n",
    "optimizer = tf.keras.optimizers.Adam(LR)\n",
    "\n",
    "checkpoint_dir = 'output/model/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, hist_embed=hist_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = TOTAL_EPOCHS\n",
    "\n",
    "log_file = open(\"log/log\", \"w\", buffering=1)\n",
    "\n",
    "best_hit = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    start = time.time()\n",
    "    all_loss_list = list()\n",
    "    db_loss_list = list()\n",
    "    dh_loss_list = list()\n",
    "    mbr_loss_list = list()\n",
    "    \n",
    "    for (batch, ent_pair) in enumerate(train_batch):\n",
    "        kg1_data, kg2_data, neg_data = get_batch_data(ent_pair.numpy())\n",
    "\n",
    "        kg1_attr_keys, kg1_attr_vals, kg1_node_list, kg1_adj_matrix = kg1_data\n",
    "        kg2_attr_keys, kg2_attr_vals, kg2_node_list, kg2_adj_matrix = kg2_data\n",
    "        neg_attr_keys, neg_attr_vals, neg_node_list, neg_adj_matrix = neg_data\n",
    "    \n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            kg1_att_embed, kg1_node_embed, kg1_states, _, _ = encoder(kg1_attr_keys, kg1_attr_vals, kg1_node_list, kg1_adj_matrix)\n",
    "            kg2_att_embed, kg2_node_embed, kg2_states, _, _ = encoder(kg2_attr_keys, kg2_attr_vals, kg2_node_list, kg2_adj_matrix)\n",
    "            neg_att_embed, neg_node_embed, neg_states, _, _ = encoder(neg_attr_keys, neg_attr_vals, neg_node_list, neg_adj_matrix)\n",
    "\n",
    "            # contrastive loss\n",
    "            kg1_final_embed = tf.concat([kg1_att_embed], axis=-1)\n",
    "            kg2_final_embed = tf.concat([kg2_att_embed], axis=-1)\n",
    "            neg_final_embed = tf.concat([neg_att_embed], axis=-1)\n",
    "            m_loss = margin_based_ranking_loss(kg1_final_embed, kg2_final_embed, neg_final_embed)\n",
    "\n",
    "            #distillation batch embeddings\n",
    "            kg1_batch_embed = hist_embed.get_batch_embed(kg1_node_list)\n",
    "            kg2_batch_embed = hist_embed.get_batch_embed(kg2_node_list)\n",
    "            neg_batch_embed = hist_embed.get_batch_embed(neg_node_list)\n",
    "            db_loss1 = distill_loss(kg1_att_embed, kg1_batch_embed[:BATCH_SIZE,:])\n",
    "            db_loss2 = distill_loss(kg2_att_embed, kg2_batch_embed[:BATCH_SIZE,:])\n",
    "            db_loss3 = distill_loss(neg_att_embed, neg_batch_embed[:BATCH_SIZE,:])\n",
    "            db_loss = (db_loss1 + db_loss2 + db_loss3)/3\n",
    "            \n",
    "            #distillation hist embeddings\n",
    "            kg1_hist_embed = hist_embed.get_hist_embed(kg1_node_list)\n",
    "            kg2_hist_embed = hist_embed.get_hist_embed(kg2_node_list)\n",
    "            neg_hist_embed = hist_embed.get_hist_embed(neg_node_list)\n",
    "            dh_loss1 = distill_loss(kg1_node_embed, kg1_hist_embed[:BATCH_SIZE,:])\n",
    "            dh_loss2 = distill_loss(kg2_node_embed, kg2_hist_embed[:BATCH_SIZE,:])\n",
    "            dh_loss3 = distill_loss(neg_node_embed, neg_hist_embed[:BATCH_SIZE,:])\n",
    "            dh_loss = (dh_loss1 + dh_loss2 + dh_loss3)/3\n",
    "\n",
    "            the_loss = m_loss + db_loss + dh_loss\n",
    "            \n",
    "            # to ensure closeness of historical embeddings\n",
    "            aux_loss = 0\n",
    "            for i in range(1, len(kg1_states)):\n",
    "                aux_loss += tf.nn.l2_loss((kg1_states[i] + GAMMA) - kg1_states[i-1]) \\\n",
    "                            + tf.nn.l2_loss((kg2_states[i] + GAMMA) - kg2_states[i-1]) \\\n",
    "                            + tf.nn.l2_loss((neg_states[i] + GAMMA) - neg_states[i-1])\n",
    "            the_loss = the_loss + (BETA * aux_loss)\n",
    "            \n",
    "            all_loss_list.append(the_loss.numpy())\n",
    "            db_loss_list.append(db_loss.numpy())\n",
    "            dh_loss_list.append(dh_loss.numpy())\n",
    "            mbr_loss_list.append(m_loss.numpy())\n",
    "\n",
    "        variables = encoder.variables + hist_embed.variables\n",
    "        gradients = tape.gradient(the_loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        if (batch+1) % 10 == 0:\n",
    "            log_str = ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1, batch+1, mean(all_loss_list)))\n",
    "            log_str += (' MBRL {:.4f} DB {:.4f} DH {:.4f}'.format(mean(mbr_loss_list), mean(db_loss_list), mean(dh_loss_list)))\n",
    "            print (log_str)\n",
    "            log_file.write(log_str+\"\\n\")\n",
    "    \n",
    "    if (epoch+1)%10 == 0:\n",
    "        hit1, hit10 = test()\n",
    "        log_str = ('Epoch {} Hit-1 {:4f} Hit-10 {:.4f}'.format(epoch+1, hit1, hit10))\n",
    "        print (log_str)\n",
    "        log_file.write(log_str+\"\\n\")\n",
    "        \n",
    "        if hit1 > best_hit:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            best_hit = hit1\n",
    "    \n",
    "    log_str = ('Epoch {} Loss {:.4f}'.format(epoch + 1, mean(all_loss_list)))\n",
    "    print (log_str)\n",
    "    log_file.write(log_str+\"\\n\")\n",
    "    log_str = ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    print (log_str)\n",
    "    log_file.write(log_str+\"\\n\")\n",
    "print (\"DONE\")\n",
    "log_file.write(\"DONE\\n\")\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD EXISTING MODEL\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
